# CS121 Linear regression
# General purpose model representation and selection code
#NAME: ALEISTER MONTFORT
#CNETID: 12174240

import numpy as np
import matplotlib.pylab as plt
import math
from asserts import assert_Xy, assert_Xbeta
#from dataset import DataSet

#############################
#                           #
#  Our code: DO NOT MODIFY  #
#                           #
#############################


def prepend_ones_column(A):
    """
    Add a ones column to the left side of an array
    """
    ones_col = np.ones((A.shape[0], 1))
    return np.hstack([ones_col, A])


def linear_regression(X, y):
    """
    Compute linear regression. Finds model, beta, that minimizes
    X*beta - y in a least squared sense.

    Inputs:
        X: (2D Numpy array of floats) predictor/independent variables
        y: (1D Numpy array) dependent variable

    Returns: Numpy array beta, which is used only by apply_beta

    Examples
    --------
    >>> X = np.array([[5, 2], [3, 2], [6, 2.1], [7, 3]]) # predictors
    >>> y = np.array([5, 2, 6, 6]) # dependent
    >>> beta = linear_regression(X, y)  # compute the coefficients
    >>> beta
    array([ 1.20104895,  1.41083916, -1.6958042 ])
    >>> apply_beta(beta, X) # apply the function defined by beta
    array([ 4.86363636,  2.04195804,  6.1048951 ,  5.98951049])
    """
    assert_Xy(X, y, fname='linear_regression')

    X_with_ones = prepend_ones_column(X)

    # Do actual computation
    beta = np.linalg.lstsq(X_with_ones, y)[0]

    return beta


def apply_beta(beta, X):
    '''
    Apply beta, the function generated by linear_regression, to the
    specified values

    Inputs:
      beta: beta as returned by linear_regression
      X: (2D Numpy array of floats) predictor/independent variables

    Returns:
      result of applying beta to the data, as an array.

      Given:
        beta = array([B0, B1, B2,...BK])
        X = array([[x11, x12, ..., x0K],
                   [x21, x22, ..., x1K],
                   ...
                   [xN1, xN2, ..., xNK]])

      result will be:
        array([B0+B1*x11+B2*x12+...+BK*x1K,
               B0+B1*x21+B2*x22+...+BK*x2K,
               ...
               B0+B1*xN1+B2*xN2+...+BK*xNK])
    '''
    assert_Xbeta(X, beta, fname='apply_beta')

    # Add a column of ones
    X_incl_ones = prepend_ones_column(X)

    # Calculate X*beta
    yhat = np.dot(X_incl_ones, beta)
    return yhat


###############
#             #
#  Your code  #
#             #
###############
class DataSet(object):
    '''
    Class for representing a data set.
    '''

    def __init__(self, dir_path):
        '''
        Constructor

        Inputs:
            dir_path: (string) path to the directory that contains the
              file
        '''
        self.dir_path = dir_path
        get_data = load_numpy_array(dir_path,"data.csv")
        get_parameters = load_json_file(dir_path, "parameters.json")
        self.predictor_vars = get_parameters["predictor_vars"]
        self.name = get_parameters["name"]
        self.dependent_var = get_parameters["dependent_var"]
        train_test = train_test_split(get_data[1], test_size=None,\
                        train_size = get_parameters["training_fraction"],\
                         random_state = get_parameters["seed"])
        self.training_data = train_test[0]
        self.test_data = train_test[1]
        self.labels = get_data[0]






####################################


class Model(object):
    def __init__(self, dataset, pred_vars):
        '''
        Construct a data structure to hold the model.

        Inputs:
            dataset: a dataset instance
            pred_vars: a list of the indices for the columns used in
              the model.
        '''

        self.dataset = dataset
        self.pred_vars = pred_vars
        # Attributes relative to data and data split
        data_t = dataset.training_data
        data_test = dataset.test_data
        X = data_t[:, pred_vars]
        y = data_t[:, dataset.dependent_var]
        X_test = data_test[:, pred_vars]
        y_test = data_test[:, dataset.dependent_var]

        # Attributes relative to model parameters and calculations of
        #determination coefficients (R square)
            # I decided to include attributes for different data sections
        self.betas = linear_regression(X,y)
        self.test_betas = linear_regression(X_test, y_test)
        self.yhat = apply_beta(self.betas, X)
        self.yhat_test = apply_beta(self.test_betas,X_test)

        # Calculation of R square for different datasets
        self.rsquare = 1-((var(y-self.yhat))/var(y))
        self.adjrsquare = self.rsquare - (1-self.rsquare)*((len(self.betas)-1)\
                                       /(len(self.yhat)-((len(self.betas)-2))))
        self.test_rsq = 1-((var(y_test-self.yhat_test)/var(y_test)))

        self.labels = dataset.labels

    def __repr__(self):
        '''
        Represents the object Model
        Inputs: Model parameters that are relevant to represent the model
        Returns: String representation of the model
        '''
        output = ((self.labels[-1]) + "~ " + str(round(self.betas[0],6)))
        betas = [self.betas[1:]]
        ls = []
        for i in self.pred_vars:
            ls.append(self.labels[i])
        import itertools
        tuple_lab_pred = [list(i) for i in zip(betas[0], ls)]
        for pair in tuple_lab_pred:
            a = (" + " + str((round(pair[0],6))) + "*" + (pair[1]))
            output = output + a
        return output + "\n" + "R2:" +str(self.rsquare)


def var(y):
    '''
    Auxiliar function
    Returns the variance of an array
    Inputs: array
    Returns: Float
    '''
    dev = np.array(y-np.mean(y))
    dev_sq = dev**2
    return dev_sq.sum()/len(y)

  #######################################
  ###############TASKS##################

def task_1a(data):
    '''
    Computes univariate models for all k predictor variables in the dataset
    Inputs: Dataset with training portion of data
    Returns: List with k Models along with their R2 value
    '''
    list_of_models = []
    for element in data.predictor_vars:
        model = Model(data, [element])
        list_of_models.append(model)
    return list_of_models

def task_1b(data):
    '''
    Computes a single model that uses all of the datasetâ€™s predictor variables
    to predict the dependent variable.
    Input: Dataset with training portion of data
    Returns: Representation object of a multivariate model along with its R2
    value.
    '''
    model = Model(data,data.predictor_vars[:])
    return model

def task2(data, n = 2):
    '''
    Tests all possible bivariate models (K = 2) and determine the one with
    the highest R2 value.

    Inputs: Dataset with training portion of data
            Combinations of n=2 independent variables to test in the models.
    Returns: Representation object of a multivariate model along with its R2
    value.
    '''
    from itertools import combinations
    list_of_pairs = list(combinations(data.predictor_vars, n))   # Here I create combinations
                                                                 # of size n from the matrix of
                                                                 # k independent variables
    matrix = []
    for pair in list_of_pairs:
        model_with_pairs = Model(data,pair)                      # I run the models for each pair, create
        tuple_model = (pair, model_with_pairs.rsquare)           # a matrix with models and its R2, and
        matrix.append(tuple_model)                               # select the model with the highest R square
    max_pair = max(matrix, key = lambda x:x[1])
    best_model = Model(data,max_pair[0])
    return best_model

def task3(data):
    '''
    Implements a heuristic known as Backward elimination. It starts with a set that contains
    all potential predictor variables and then repeatedly eliminates variables until the set
    contains K variables. At each step, the algorithm identifies the variable in the
    model that when subtracted, yields the model with the best R2 value and eliminate it
    from the set.
    Inputs: DataSet
    Returns: A list with the best model for K independent variables
    '''
    ls1 = data.predictor_vars
    list_of_optimals = []
    list_of_models = []
    while len (ls1) >1:
        matrix = []
        for i in range(len(ls1)):              # Here I create the combinations of k-1 independent
            p = (ls1[i + 1:])                  # variables, taken from a set of k independent vars.
            q = (ls1[:i])
            l = p + q
            model = Model(data,l)              # I run the models and estimate their r square
            tuple_model = (l, model.rsquare)
            matrix.append(tuple_model)          # and generate a matrix of models and R's squares
        max_rsqmodel=max(matrix,key=lambda x:x[1])      # Finally I select the model with the largest R square
        best_model = Model(data, sorted(max_rsqmodel[0]))
        ls1 = max_rsqmodel[0]                            # And update the list of k independent variables
        list_of_optimals.append(ls1)                     # I use this list later in task 4
        list_of_models.append(best_model)
    list_of_models.reverse()                              # I reverse the list to present the univariate model
    return list_of_models                                 # first.


def task4 (data):
    '''
    Estimates the best model from Task 4 according to its
    adjusted R square
    Inputs: DataSet
    Output: The model computed using the backward elimination heuristic that
    has the largest adjusted R2 along with both its R2 and adjusted R2 values.
    '''
    ls1=data.predictor_vars                        # This is repeated code that should be included in
    list_of_optimals=[]                            # a function.
    while len (ls1) >1:
        matrix = []
        for i in range(len(ls1)):
            p = (ls1[i + 1:])
            q = (ls1[:i])
            l = p+q
            model = Model(data,l)
            tuple_model = (l, model.rsquare)
            matrix.append(tuple_model)
        max_rsqmodel=max(matrix,key=lambda x:x[1])
        best_model = Model(data, sorted(max_rsqmodel[0], reverse=True))
        ls1 = max_rsqmodel[0]
        list_of_optimals.append(ls1)

    matrix = []                                     # Here I select the model with the largest R square
    for betas in list_of_optimals:
        model = Model(data,betas)
        model.adjrsquare
        tuple_model = (betas,model.adjrsquare)
        matrix.append(tuple_model)
        max_pair = max(matrix, key = lambda x:x[1])

    best_model = Model(data,max_pair[0])

    return best_model ,"Adjusted R2:", best_model.adjrsquare

def task5 (data):
    '''
    Evaluates the selected model in Task 4  with the dataset in data_test.
    Input: Dataset
    Output: the model chosen for Task 4 along with its R2 value computed using
    the training data and its R2 value computed using the testing data.
    '''
    ls1=data.predictor_vars        # Again, this is repeated code. The only change is in the
    list_of_optimals=[]            # return statement.
    while len (ls1) >1:
        matrix = []
        for i in range(len(ls1)):
            p = (ls1[i + 1:])
            q = (ls1[:i])
            l = p+q
            model = Model(data,l)
            tuple_model = (l, model.rsquare)
            matrix.append(tuple_model)
        max_rsqmodel=max(matrix,key=lambda x:x[1])
        best_model = Model(data, sorted(max_rsqmodel[0], reverse=True))
        ls1 = max_rsqmodel[0]
        list_of_optimals.append(ls1)

    matrix = []
    for betas in list_of_optimals:
        model = Model(data,betas)
        model.adjrsquare
        tuple_model = (betas,model.adjrsquare)
        matrix.append(tuple_model)
        max_pair = max(matrix, key = lambda x:x[1])
    best_model = Model(data,max_pair[0])
    return best_model, "Testing R2:", best_model.test_rsq
